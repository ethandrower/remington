#!/usr/bin/env python3
"""
Comprehensive Test Suite for PM Agent Polling Monitors
Tests real API integration, polling detection, and Claude Code processing
"""

import pytest
import os
import sys
import time
import sqlite3
from pathlib import Path
from unittest.mock import Mock, patch, MagicMock
from datetime import datetime, timedelta
from dotenv import load_dotenv

# Load environment
load_dotenv()

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from src.monitors.slack_monitor import SlackMonitor
from src.monitors.jira_monitor import JiraMonitor
from src.monitors.bitbucket_monitor import BitbucketMonitor
from src.orchestration.claude_code_orchestrator import ClaudeCodeOrchestrator


class TestSlackMonitorRealAPI:
    """Test Slack Monitor with real API calls"""

    def test_slack_monitor_initialization(self):
        """Test Slack monitor can initialize with real credentials"""
        try:
            monitor = SlackMonitor()
            assert monitor.slack_token is not None
            assert monitor.target_channel is not None
            assert monitor.bot_user_id is not None
            assert monitor.db_path.exists()
            print(f"‚úÖ Slack Monitor initialized: Channel={monitor.target_channel}")
        except ValueError as e:
            pytest.skip(f"Slack not configured: {e}")

    def test_slack_poll_for_mentions(self):
        """Test Slack polling returns list (may be empty)"""
        try:
            monitor = SlackMonitor()
            events = monitor.poll_for_mentions()
            assert isinstance(events, list), "poll_for_mentions should return a list"
            print(f"‚úÖ Slack polling works: Found {len(events)} event(s)")

            # Check database tracking
            conn = sqlite3.connect(monitor.db_path)
            cursor = conn.execute("SELECT COUNT(*) FROM processed_messages")
            count = cursor.fetchone()[0]
            conn.close()
            print(f"   Database tracking: {count} messages processed")

        except ValueError as e:
            pytest.skip(f"Slack not configured: {e}")

    def test_slack_no_reprocessing(self):
        """Test that Slack doesn't reprocess same message"""
        try:
            monitor = SlackMonitor()

            # First poll
            events1 = monitor.poll_for_mentions()

            # Immediate second poll (should return same items or empty if already processed)
            events2 = monitor.poll_for_mentions()

            # If we got events, they should be tracked
            if events1:
                # Second poll should not return already-processed items
                assert len(events2) == 0 or events2 != events1, \
                    "Should not reprocess same messages"

            print(f"‚úÖ Slack deduplication works: Poll1={len(events1)}, Poll2={len(events2)}")

        except ValueError as e:
            pytest.skip(f"Slack not configured: {e}")


class TestJiraMonitorRealAPI:
    """Test Jira Monitor with real API calls"""

    def test_jira_monitor_initialization(self):
        """Test Jira monitor can initialize with real credentials"""
        try:
            monitor = JiraMonitor()
            assert monitor.cloud_id is not None
            assert monitor.project_key is not None
            assert monitor.db_path.exists()
            print(f"‚úÖ Jira Monitor initialized: Project={monitor.project_key}")
        except ValueError as e:
            pytest.skip(f"Jira not configured: {e}")

    def test_jira_poll_for_mentions(self):
        """Test Jira polling returns list"""
        try:
            monitor = JiraMonitor()
            events = monitor.poll_for_mentions()
            assert isinstance(events, list), "poll_for_mentions should return a list"
            print(f"‚úÖ Jira polling works: Found {len(events)} event(s)")

            if events:
                # Verify event structure
                event = events[0]
                assert 'issue_key' in event
                assert 'text' in event
                assert 'author' in event
                print(f"   Sample event: {event['issue_key']} by {event['author']}")

        except ValueError as e:
            pytest.skip(f"Jira not configured: {e}")

    def test_jira_no_reprocessing(self):
        """Test that Jira doesn't reprocess same comments"""
        try:
            monitor = JiraMonitor()

            # First poll
            events1 = monitor.poll_for_mentions()

            # Immediate second poll
            events2 = monitor.poll_for_mentions()

            # Should not return duplicates
            if events1:
                assert len(events2) == 0, "Should not reprocess same comments"

            print(f"‚úÖ Jira deduplication works: Poll1={len(events1)}, Poll2={len(events2)}")

        except ValueError as e:
            pytest.skip(f"Jira not configured: {e}")


class TestBitbucketMonitorRealAPI:
    """Test Bitbucket Monitor with real API calls"""

    def test_bitbucket_monitor_initialization(self):
        """Test Bitbucket monitor can initialize"""
        try:
            monitor = BitbucketMonitor()
            assert monitor.workspace is not None
            assert len(monitor.repos) > 0
            assert monitor.db_path.exists()
            print(f"‚úÖ Bitbucket Monitor initialized: Workspace={monitor.workspace}, Repos={monitor.repos}")
        except ValueError as e:
            pytest.skip(f"Bitbucket not configured: {e}")

    def test_bitbucket_database_structure(self):
        """Test Bitbucket database has correct structure"""
        try:
            monitor = BitbucketMonitor()

            conn = sqlite3.connect(monitor.db_path)
            cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table'")
            tables = [row[0] for row in cursor.fetchall()]
            conn.close()

            assert 'processed_pr_comments' in tables
            assert 'last_check_per_repo' in tables
            print(f"‚úÖ Bitbucket database structure correct: {tables}")

        except ValueError as e:
            pytest.skip(f"Bitbucket not configured: {e}")


class TestClaudeCodeOrchestrator:
    """Test Claude Code Orchestrator"""

    def test_orchestrator_initialization(self):
        """Test Claude Code orchestrator initializes"""
        try:
            orchestrator = ClaudeCodeOrchestrator()
            assert orchestrator.settings_file.exists()
            print(f"‚úÖ Claude Code Orchestrator initialized")
            print(f"   Settings: {orchestrator.settings_file}")
        except ValueError as e:
            pytest.skip(f"Claude Code not available: {e}")

    def test_orchestrator_can_process_jira_comment(self):
        """Test orchestrator can process a Jira comment (mocked issue)"""
        try:
            orchestrator = ClaudeCodeOrchestrator()

            # This will attempt to call Claude Code
            # It might fail if issue doesn't exist, but we're testing the pipeline
            result = orchestrator.process_jira_comment(
                issue_key="TEST-1",
                comment_text="This is a test comment to verify the orchestrator works",
                commenter="Test User",
                agent_type="jira-manager"
            )

            assert result['status'] == 'complete'
            assert result['agent_used'] == 'jira-manager'
            assert 'response' in result

            print(f"‚úÖ Claude Code processing works")
            print(f"   Response length: {len(result['response'])} chars")

        except ValueError as e:
            pytest.skip(f"Claude Code not available: {e}")
        except Exception as e:
            # Claude Code might fail on non-existent issue, but pipeline should work
            print(f"‚ö†Ô∏è  Claude Code invoked but may have failed on test issue: {e}")


class TestEndToEndPollingFlow:
    """Test complete end-to-end polling flow"""

    def test_slack_to_orchestrator_flow(self):
        """Test: Slack poll ‚Üí detect new ‚Üí process with Claude Code"""
        try:
            monitor = SlackMonitor()
            orchestrator = ClaudeCodeOrchestrator()

            # Poll for mentions
            events = monitor.poll_for_mentions()

            print(f"üìä Slack E2E Test: Found {len(events)} event(s)")

            if events:
                # Process first event with orchestrator
                event = events[0]
                print(f"   Processing: {event['text'][:100]}...")

                # Note: SlackMonitor has its own Claude Code invocation
                # This test verifies the flow exists
                assert 'user' in event
                assert 'text' in event
                assert 'ts' in event

                print(f"‚úÖ Slack ‚Üí Orchestrator flow validated")
            else:
                print(f"   No new events to process (expected if no recent mentions)")

        except ValueError as e:
            pytest.skip(f"Components not configured: {e}")

    def test_jira_to_orchestrator_flow(self):
        """Test: Jira poll ‚Üí detect new ‚Üí process with Claude Code"""
        try:
            monitor = JiraMonitor()
            orchestrator = ClaudeCodeOrchestrator()

            # Poll for mentions
            events = monitor.poll_for_mentions()

            print(f"üìä Jira E2E Test: Found {len(events)} event(s)")

            if events:
                # Process first event
                event = events[0]
                print(f"   Processing: {event['issue_key']} - {event['text'][:100]}...")

                # Actually invoke orchestrator
                result = orchestrator.process_jira_comment(
                    issue_key=event['issue_key'],
                    comment_text=event['text'],
                    commenter=event['author']
                )

                assert result['status'] == 'complete'
                print(f"‚úÖ Jira ‚Üí Orchestrator flow complete")
                print(f"   Issue: {event['issue_key']}")
                print(f"   Response: {len(result['response'])} chars")
            else:
                print(f"   No new comments to process (expected if no recent activity)")

        except ValueError as e:
            pytest.skip(f"Components not configured: {e}")


class TestDatabaseTracking:
    """Test database tracking prevents reprocessing"""

    def test_slack_database_isolation(self):
        """Test each monitor has isolated database"""
        try:
            slack_monitor = SlackMonitor()
            jira_monitor = JiraMonitor()

            assert slack_monitor.db_path != jira_monitor.db_path
            assert slack_monitor.db_path.exists()
            assert jira_monitor.db_path.exists()

            print(f"‚úÖ Database isolation verified")
            print(f"   Slack DB: {slack_monitor.db_path}")
            print(f"   Jira DB: {jira_monitor.db_path}")

        except ValueError as e:
            pytest.skip(f"Monitors not configured: {e}")

    def test_processed_items_persisted(self):
        """Test processed items are persisted across monitor instances"""
        try:
            # Create first monitor instance
            monitor1 = JiraMonitor()
            events1 = monitor1.poll_for_mentions()

            # Create second monitor instance (simulates restart)
            monitor2 = JiraMonitor()
            events2 = monitor2.poll_for_mentions()

            # Should not reprocess (database persists)
            if events1:
                assert len(events2) == 0, "Items should remain processed across restarts"

            print(f"‚úÖ Database persistence verified")
            print(f"   Instance 1: {len(events1)} events")
            print(f"   Instance 2: {len(events2)} events (should be 0)")

        except ValueError as e:
            pytest.skip(f"Jira not configured: {e}")


class TestPollingPerformance:
    """Test polling performance (speed, API calls)"""

    def test_slack_polling_speed(self):
        """Test Slack polling completes quickly"""
        try:
            monitor = SlackMonitor()

            start_time = time.time()
            events = monitor.poll_for_mentions()
            elapsed = time.time() - start_time

            # Should complete in < 5 seconds
            assert elapsed < 5.0, f"Polling too slow: {elapsed:.2f}s"

            print(f"‚úÖ Slack polling speed: {elapsed:.2f}s for {len(events)} events")

        except ValueError as e:
            pytest.skip(f"Slack not configured: {e}")

    def test_jira_polling_speed(self):
        """Test Jira polling completes quickly"""
        try:
            monitor = JiraMonitor()

            start_time = time.time()
            events = monitor.poll_for_mentions()
            elapsed = time.time() - start_time

            # Should complete in < 5 seconds
            assert elapsed < 5.0, f"Polling too slow: {elapsed:.2f}s"

            print(f"‚úÖ Jira polling speed: {elapsed:.2f}s for {len(events)} events")

        except ValueError as e:
            pytest.skip(f"Jira not configured: {e}")


class TestMockedResponses:
    """Test Claude Code effectiveness with mocked API responses"""

    def test_mocked_jira_comment_processing(self):
        """Test orchestrator with mocked Jira API response"""
        # This would require mocking the MCP tools
        # For now, we'll test the prompt construction
        try:
            orchestrator = ClaudeCodeOrchestrator()

            prompt = orchestrator._build_jira_prompt(
                issue_key="ECD-999",
                comment_text="This PR has been waiting 3 days for review",
                commenter="Sarah Johnson",
                agent_type="jira-manager"
            )

            # Verify prompt includes required elements
            assert "ECD-999" in prompt
            assert "Sarah Johnson" in prompt
            assert "3 days" in prompt
            assert ".claude/agents/jira-manager.md" in prompt
            assert "mcp__atlassian__getJiraIssue" in prompt

            print(f"‚úÖ Jira prompt construction validated")
            print(f"   Length: {len(prompt)} chars")

        except ValueError as e:
            pytest.skip(f"Claude Code not available: {e}")


# Test runner
if __name__ == "__main__":
    print("\n" + "="*70)
    print(" PM Agent - Comprehensive Polling Monitor Test Suite ".center(70))
    print("="*70 + "\n")

    # Run with verbose output
    pytest.main([__file__, "-v", "-s", "--tb=short"])
